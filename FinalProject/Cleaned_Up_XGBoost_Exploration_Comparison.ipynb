{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Benchmarking XGBoost Against Other Machine Learning Models\n",
    "\n",
    "---\n",
    "\n",
    "## Part I: Understanding XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "- Briefly introduce machine learning and the role of ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background on Boosting\n",
    "- Explain the concept of boosting in machine learning. Refresh the concept of boosing with an image depiction?\n",
    "    - Boosting is an ensemble method that trains sequential models on the errors, or residuals, of previous models, thus creating a strong learner via iteration on many weak learners. **ISL**  **& BOOK 92**\n",
    "\n",
    "- Historical evolution leading to gradient boosting.\n",
    "    - Boositng and other ensemble methods were popularized due standard decision trees overfitting data. Boosting allowed for a more generalizable model with \"consistency and power\". **BOOK 110**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Overview\n",
    "- Detailed explanation of XGBoost and its core algorithm.\n",
    "    - XGBoost, which stands for \"Extreme Gradient Boosting\", is an open source software who's algorithm uses gradient boosting to . Averages or the majority class are considered when outputing a result. **Read the docs: tutorials**\n",
    "\n",
    "- Advantages of XGBoost over other boosting methods.\n",
    "    - XGBoost is extremely fast, memory efficient, and can perform advance model operations **Read the docs: faq** **& Book 57**\n",
    "    - Not only is XGBoost fast, but it can provide better accuracy than other gradient boosting models. Speed can be attributed to it's ability to handle missing values, it storing sparce matrices of data, computing in parallel with blocks, being cache-aware. **Book 129**  \n",
    "        - missing values?????\n",
    "        -  from original XGBoost papaer: A Scalable Tree Boosting System, the sparsity-aware split-finding algorithm performed 50 times faster than the standard approach on the All-State-10K dataset.**Book 112**\n",
    "        - Parallel computing occurs when multiple computational units are working together on the same problem at the same time. XGBoost sorts and compresses the data into blocks. These blocks may be distributed to multiple machines, or to external memory (out of core). Sorting the data is faster with blocks. The split-finding algorithm takes advantage of blocks and the search for quantiles is faster due to blocks. In each of these cases, XGBoost provides parallel computing to expedite the model-building process. **Book 112**\n",
    "        - According to XGBoost: A Scalable Tree Boosting System, prefetching lengthens read/write dependency and reduces runtimes by approximately 50% for datasets with a large number of rows. **BOOK 112**\n",
    "    - And accuracy can be attributed to XGBoost's built-in regularization. \n",
    "        - Regularization is the process of adding information to reduce variance and prevent overfitting. **BOOK 112**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts and Features of XGBoost\n",
    "- Discuss tree boosting, regularized learning, and model complexity.\n",
    "- Overview of handling missing data, parallel processing, and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Parameters\n",
    "- List and explain crucial XGBoost hyperparameters.\n",
    "    -  XGBoost is easy to use with scikit-learn since \"all standard methods are available\" and scikit-learn's functions like train_test_split, and GridSearchCV can be combined with XGBoost models. **BOOK 117**\n",
    "    - Some hyperparameters are required to build a specific model. \n",
    "        - General parameters determine what booster we will be using. **Read the docs: parameter**\n",
    "        - Booster parameters will depend on the general parameter chosen. **Read the docs: parameter**\n",
    "        - Learning task parameters determine the \"objective\", or scoring metric, and the evaluation metric. **Read the docs: parameter**\n",
    "    - These hyperparameters \"are designed to improve upon accuracy and speed. **Book 130**\n",
    "\n",
    "    - Hyperparameters of specific models are similar to scikit-learn's hyperparameters:\n",
    "\n",
    "<img src=\"../images/XGBoost_hyperparameters.png\" alt=\"Some of XGBoost's core hyperparameters\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "- Show how these parameters can affect model performance. **Code for this?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation and Setup\n",
    "- ~~Guide on setting up XGBoost in a development environment.~~ May take up too much time. Perhaps link the documentation page on how to install and discuss to install via pip or conda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "- Discuss the preprocessing required for optimal XGBoost performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with XGBoost\n",
    "- Step-by-step process of training an XGBoost model.\n",
    "- Techniques for evaluating model performance. \n",
    "\n",
    "- **Do this in the code block above where we're demo-ing parameters and hyperparameters**\n",
    "    - page 119 for step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "- How to interpret model outputs, importance scores, and diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Performance Comparison of XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Goals\n",
    "- Define the objectives of the performance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Selection of Competing Models\n",
    "- Choose a set of models for comparison (e.g., Random Forest, SVM, Neural Networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "- Introduce the dataset(s) used for the comparison.\n",
    "- Include feature descriptions and any preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "- Define the metrics for evaluating model performance (e.g., accuracy, F1 score, ROC-AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Strategy\n",
    "- Explain the cross-validation process to ensure fairness in comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "- How each model's hyperparameters are tuned for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "- Train the selected models on the dataset.\n",
    "- Evaluate and compare their performance using the defined metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis\n",
    "- Present the comparison results in tables or graphs.\n",
    "- Statistical tests, if applicable, to establish significant differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "- Interpret the comparison findings.\n",
    "- Discuss where XGBoost outperforms or underperforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- Summarize key takeaways from the XGBoost exploration and model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices and Supporting Materials\n",
    "\n",
    "- Code snippets, Jupyter Notebook links, or GitHub repository.\n",
    "- Detailed tables and graphical representations of results.\n",
    "- Additional notes on the computational environment, data access, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sportsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Benchmarking XGBoost Against Other Machine Learning Models\n",
    "\n",
    "---\n",
    "\n",
    "## Part I: Understanding XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "- Briefly introduce machine learning and the role of ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background on Boosting\n",
    "- ~~Explain the concept of boosting in machine learning.~~ Refresh the concept of boosing with an image depiction?\n",
    "- ~~Historical evolution leading to gradient boosting.~~ This may take up too much time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "- Decision trees produced machine learning models that were too accurate and failed to generalize well to new data. Ensemble methods proved more effective by combining many decision trees via bagging and boosting. A leading algorithm that emerged from the tree ensemble trajectory was gradient boosting. **BOOK pg110**\n",
    "\n",
    "- The consistency, power, and outstanding results of gradient boosting convinced Tianqi Chen from the University of Washington to enhance its capabilities. He called the new algorithm XGBoost, short for Extreme Gradient Boosting. Chen's new form of gradient boosting included built-in regularization and impressive gains in speed. **BOOK p110**\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "- fresh reminder on what boosting is: Boosting ... trees are\n",
    "grown sequentially: each tree is grown using information from previously.   Given the current\n",
    "model, we fit a decision tree to the residuals from the model. **ISL**\n",
    "- The general idea behind boosting is to transform weak learners into strong learners by iteratively improving upon errors. The key idea behind gradient boosting is to use gradient descent to minimize the errors of the residuals. **BOOK page 35**\n",
    "- boosting approaches are known to \"learn slowly\" **ISL**\n",
    "    - images too\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Overview\n",
    "- Detailed explanation of XGBoost and its core algorithm.\n",
    "- Advantages of XGBoost over other boosting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "- XGBoost, which stands for \"Extreme Gradient Boosting\", is an open source software (????) and it's algorithm uses the boosting technique. Averages or the majority class are considered when outputing a result. **https://xgboost.readthedocs.io/en/stable/tutorials/model.html**\n",
    "\n",
    "- decision tree ensemble with gradient boosting: **https://xgboost.readthedocs.io/en/stable/tutorials/model.html**\n",
    "- by building decision trees, the base learners of XGBoost machine learning models **BOOK p57**\n",
    "- XGBoost is an ensemble method, meaning that it is composed of different machine learning models that combine to work together.\n",
    "The individual models that make up the ensemble in XGBoost are called base learners.\n",
    "Decision trees, the most commonly used XGBoost base learners, are unique in the machine learning landscape. **BOOK p 58**\n",
    "-  Another solution is to aggregate the predictions of many trees, a strategy that Random Forests and XGBoost employ. **BOOK p 58**\n",
    "- In the next chapter, you will learn how to build Random Forests, our first ensemble method and a rival of XGBoost. The applications of Random Forests are important for comprehending the difference between bagging and boosting, generating machine learning models comparable to XGBoost, and learning about the limitations of Random Forests that facilitated the development of XGBoost in the first place. **BOOK p 77**\n",
    "- Like XGBoost, random forests are ensembles of decision trees. The difference is that random forests combine trees via bagging, while XGBoost combines trees via boosting. Random forests are a viable alternative to XGBoost with advantages and limitations that are highlighted in this chapter. **BOOK p 78**\n",
    "- In this chapter, you will discover the power behind gradient boosting, which is at the core of XGBoost **BOOK p 92**\n",
    "- Boosting, by contrast, learns from the mistakes of individual trees. The general idea is to adjust new trees based on the errors of previous trees. **BOOK p 92**\n",
    "- A weak learner is a machine learning algorithm that barely performs better than chance. By contrast, a stronger learner has learned a considerable amount from data and performs quite well. **BOOK p 92**\n",
    "- Gradient boosting computes the residuals of each tree's predictions and sums all the residuals to score the model.... XGBoost, an advanced version of gradient boosting. **BOOK p 93**\n",
    "- \n",
    "\n",
    "\n",
    "- Advantages of XGBoost over other boosting methods:\n",
    "    - \"designed to be memory efficeint\" and can generally perform as long as the data fit into memory **https://xgboost.readthedocs.io/en/stable/faq.html**\n",
    "    - Normally, XGBoost is very fast. In fact, it has a reputation for being the fastest boosting ensemble method out there, a reputation that we will check in this book! **BOOK p 57**\n",
    "\n",
    "- while xgboost can perform many advanced model operations, we will be focusing on the regression model for XGBoost\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**KEY ADVANCEMENTS**\n",
    "\n",
    "In this chapter, you learned how XGBoost was designed to improve the accuracy and speed of gradient boosting with missing values, sparse matrices, parallel computing, sharding, and blocking **BOOK p129**\n",
    "\n",
    "- handling missing values:\n",
    "    - missing hyperparameter that can be set to any value. When given a missing data point, XGBoost scores different split options and chooses the one with the best results. **BOOK p111**\n",
    "\n",
    "- speed:\n",
    "    - The following new design features give XGBoost a big edge in speed over comparable ensemble algorithms:\n",
    "        - Approximate split-finding algorithm \n",
    "            - XGBoost presents an exact greedy algorithm in addition to a new approximate split-finding algorithm. The split-finding algorithm uses quantiles, percentages that split data, to propose candidate splits. In a global proposal, the same quantiles are used throughout the entire training, and in a local proposal, new quantiles are provided for each round of splitting.\n",
    "\n",
    "            A previously known algorithm, quantile sketch, works well with equally weighted datasets. XGBoost presents a novel weighted quantile sketch based on merging and pruning with a theoretical guarantee. **BOOK p 111**\n",
    "\n",
    "        - Sparsity aware split-finding\n",
    "            - Sparse matrices are designed to only store data points with non-zero and non-null values. This saves valuable space. A sparsity- aware split indicates that when looking for splits, XGBoost is faster because its matrices are sparse.\n",
    "           \n",
    "            According to the original paper, XGBoost: A Scalable Tree Boosting System, the sparsity-aware split-finding algorithm performed 50 times faster than the standard approach on the All-State-10K dataset. **BOOK 112**\n",
    "\n",
    "            one hot encoder makes a lot of 0s...\n",
    "\n",
    "        - Parallel computing\n",
    "            - Boosting is not ideal for parallel computing since each tree depends on the results of the previous tree. There are opportunities,\n",
    "            however, where parallelization may take place.\n",
    "            \n",
    "            Parallel computing occurs when multiple computational units are working together on the same problem at the same time. XGBoost sorts and compresses the data into blocks. These blocks may be distributed to multiple machines, or to external memory (out of core).\n",
    "            \n",
    "            Sorting the data is faster with blocks. The split-finding algorithm takes advantage of blocks and the search for quantiles is faster due to blocks. In each of these cases, XGBoost provides parallel computing to expedite the model-building process. **BOOK p 112**\n",
    "\n",
    "        - Cache-aware access\n",
    "            - XGBoost uses cache-aware prefetching. XGBoost allocates an internal buffer, fetches the gradient statistics, and performs accumulation with mini batches. According to XGBoost: A Scalable Tree Boosting System, prefetching lengthens read/write dependency and reduces runtimes by approximately 50% for datasets with a large number of rows. **BOOK 112**\n",
    "\n",
    "\n",
    "        - Block compression and sharding\n",
    "            - XGBoost delivers additional speed gains through block compression and block sharding. \n",
    "            \n",
    "                        \n",
    "            Block compression helps with computationally expensive disk reading by compressing columns. Block sharding decreases read times by sharding the data into multiple disks that alternate when reading the data.**BOOK 112**\n",
    "\n",
    "\n",
    "- accuracy:\n",
    "    - XGBoost adds built-in regularization to achieve accuracy gains beyond gradient boosting. Regularization is the process of adding information to reduce variance and prevent overfitting.\n",
    "\n",
    "    XGBoost adds built-in regularization to achieve accuracy gains beyond gradient boosting. Regularization is the process of adding information to reduce variance and prevent overfitting. **BOOK 112**\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts and Features of XGBoost\n",
    "- Discuss tree boosting, regularized learning, and model complexity.\n",
    "- Overview of handling missing data, parallel processing, and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "- regularized parameter selection. **BOOK pg 110**\n",
    "\n",
    "- Building XGBoost models in scikit-learn is very similar to building other machine learning models in scikit-learn, as you have experienced throughout this book. All standard scikit-learn methods, such as .fit, and .predict, are available, in addition to essential tools such as train_test_split, cross_val_score, GridSearchCV, and RandomizedSearchCV. **BOOK p117**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Parameters\n",
    "- List and explain crucial XGBoost hyperparameters.\n",
    "- Show how these parameters can affect model performance. **Code for this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "- hyperparameters that must be set:\n",
    "    - General parameters relate to which booster we are using to do boosting, commonly tree or linear model\n",
    "    - Booster parameters depend on which booster you have chosen\n",
    "    - Learning task parameters decide on the learning scenario. For example, regression tasks may use different parameters with ranking tasks.\n",
    "\n",
    "    - general parameters:\n",
    "        - booster [default= gbtree ]\n",
    "        Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions.\n",
    "    - booster parameters:\n",
    "        - each booster option from above has their own parameters that consist of similar parameters you would find for each model in sklearn functions\n",
    "    - learning task parameters:\n",
    "        - objective [default=reg:squarederror ]\n",
    "            - ????????\n",
    "        - eval_metric [default according to objective]\n",
    "\n",
    "- all from **https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters**\n",
    "\n",
    "- XGBoost base learner hyperparameters incorporate all decision tree hyperparameters as a starting point. **BOOK p 130**\n",
    "- Hyperparameters unique to XGBoost are designed to improve upon accuracy and speed. **BOOK 130**\n",
    "\n",
    "- Picture from **BOOK p135** that shows key hyperparameters \n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation and Setup\n",
    "- ~~Guide on setting up XGBoost in a development environment.~~ May take up too much time. Perhaps link the documentation page on how to install and discuss to install via pip or conda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "- Discuss the preprocessing required for optimal XGBoost performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with XGBoost\n",
    "- Step-by-step process of training an XGBoost model.\n",
    "- Techniques for evaluating model performance. **maybe move this below after we've run and demonstrated a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "- page 119 for step by step\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "- How to interpret model outputs, importance scores, and diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Performance Comparison of XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Goals\n",
    "- Define the objectives of the performance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Selection of Competing Models\n",
    "- Choose a set of models for comparison (e.g., Random Forest, SVM, Neural Networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "- Introduce the dataset(s) used for the comparison.\n",
    "- Include feature descriptions and any preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "- Define the metrics for evaluating model performance (e.g., accuracy, F1 score, ROC-AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Strategy\n",
    "- Explain the cross-validation process to ensure fairness in comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "- How each model's hyperparameters are tuned for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "- Train the selected models on the dataset.\n",
    "- Evaluate and compare their performance using the defined metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis\n",
    "- Present the comparison results in tables or graphs.\n",
    "- Statistical tests, if applicable, to establish significant differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "- Interpret the comparison findings.\n",
    "- Discuss where XGBoost outperforms or underperforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- Summarize key takeaways from the XGBoost exploration and model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices and Supporting Materials\n",
    "\n",
    "- Code snippets, Jupyter Notebook links, or GitHub repository.\n",
    "- Detailed tables and graphical representations of results.\n",
    "- Additional notes on the computational environment, data access, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sportsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

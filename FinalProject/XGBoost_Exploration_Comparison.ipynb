{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exploring and Benchmarking XGBoost Against Other Machine Learning Models\n",
                "\n",
                "---\n",
                "\n",
                "## Part I: Understanding XGBoost"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### What is XGBoost?\n",
                "\n",
                "XGboost, which stands for \"Extreme Gradient Boosting\", is an open source software who's algorithm uses gradient boosting to provide better performing models. Similar to trees we have seen in class, averages or the majority class are considered when outputing a result.$^{[1]}$\n",
                "\n",
                "Gradient Boosting is an ensemble method that trains sequential models on the errors, or residuals, of previous models, thus creating a strong learner via iteration on many weak learners. $^{[2, p. 92]}$ $^{[3]}$\n",
                "\n",
                "Boositng and other ensemble methods were popularized due standard decision trees overfitting data. Boosting allowed for a more generalizable model with \"consistency and power\".$^{[2, p. 110]}$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Why XGBoost?\n",
                "\n",
                "As mentioned above, XGBoost provides better performing models. This is due to some key advantages that XGBoosts provides than standard boosting methods we have seen in scikit-learn. A few of these advantages are that XGBoost is:\n",
                "\n",
                "- extrememly fast,\n",
                "- ability to handle and learn with missing values,\n",
                "- memory efficient by storing sparce matrices,\n",
                "- computes in parallel,\n",
                "- improved accuracy, \n",
                "- and can perform advance model operations. $^{[4]}$ $^{[2, p. 57, 129]}$  \n",
                "\n",
                "While speed will vary from computer to computer, XGBoost is known for its speed. The original XGBoost paper detailed performance 50 times faster than a standard approach on the same dataset. $^{[2, p. 112]}$\n",
                "\n",
                "Speed is also due to the fact that XGBoost can compute in parallel, meaning multiple processes can be done at the same time, by putting data into \"blocks\". These blocks are then sent to multiple machines or external memory to \"expedite the model-building process\". $^{[2, p. 112]}$\n",
                "\n",
                "Improved accuracy can be attributed to XGBoost's built-in regularization where information is added to \"reduce variance and prevent overfitting\". $^{[2, p. 112]}$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### XGBoost Parameters\n",
                "\n",
                "XGBoost is easy to use with scikit-learn since \"all standard methods are available\" (such as fit) and scikit-learn's functions like train_test_split, and GridSearchCV can be combined with XGBoost models. $^{[2, p. 117]}$\n",
                "\n",
                "However, there are some differences with XGBoost, namely the hyperparameters used to build models. Some hyperparameters are required to build a specific model: \n",
                "\n",
                "- General parameters determine what booster we will be using. Default is \"gbtree\".$^{[5]}$\n",
                "- Booster parameters will depend on the general parameter chosen. These are similar to hyperparameters we've seen for different models. $^{[5]}$\n",
                "- Learning task parameters determine the \"objective\", or scoring metric, and the evaluation metric. Default is \"reg:squarederror\". $^{[5]}$\n",
                "\n",
                "XGBoost hyperparemeters \"are designed to improve upon accuracy and speed\" of existing models and tuning them can be done using scikit-learn's GridSearchCV. $^{[2, p. 130]}$ \n",
                "\n",
                "A few Hyperparameters of that are similar to scikit-learn's hyperparameters are below for reference:\n",
                "\n",
                "<img src=\"../images/XGBoost_hyperparameters.png\" alt=\"Some of XGBoost's core hyperparameters\" style=\"width: 500px;\"/>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### How to use XGBoost?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Installation and Setup\n",
                "\n",
                "Installation from XGBoost documentation:\n",
                "\n",
                "##### **Conda**:\n",
                "conda install -c conda-forge py-xgboost\n",
                "\n",
                "##### **Pip**:\n",
                "pip install xgboost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# building the model\n",
                "from xgboost import XGBClassifier\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "xgb = XGBClassifier(booster='gbtree')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# about 40 seconds to run\n",
                "\n",
                "# grid search to find the best parameters (using some parameters from above list)\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "\n",
                "param_grid = {\n",
                "    'n_estimators': [50, 100, 200, 500],\n",
                "    'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
                "    'max_depth': [2, 5, 10],\n",
                "    'min_child_weight': [1, 2, 4, 8, 16], # min_saples_leaf\n",
                "}\n",
                "\n",
                "grid_xgb = GridSearchCV(xgb, param_grid, scoring='accuracy', n_jobs=-1)\n",
                "\n",
                "grid_xgb.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid_xgb.best_params_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid_xgb.best_score_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# accuracy on training data\n",
                "xgb_best = grid_xgb.best_estimator_\n",
                "\n",
                "y_pred_train = xgb_best.predict(X_train)\n",
                "\n",
                "score = accuracy_score(y_pred_train, y_train)\n",
                "print('Training Score: ' + str(score))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# accuracy on testing data\n",
                "y_pred = xgb_best.predict(X_test)\n",
                "\n",
                "score = accuracy_score(y_pred, y_test)\n",
                "print('Testing Score: ' + str(score))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
                "\n",
                "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot(cmap='Blues')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part II: Performance Comparison of XGBoost"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <u>Benchmarking Goals</u>\n",
                "- Define the objectives of the performance comparison."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The objectives of the performance comparison are to:\n",
                "- Compare the performance of XGBoost against other machine learning models.\n",
                "- Determine the optimal hyperparameters for each model.\n",
                "- Identify the best model for the given dataset (may or may not be XGBoost)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <u>Selection of Competing Models</u>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For <u>classification</u>, we're using five machine learning models for comparison. Here is a brief description of each:\n",
                "\n",
                "1. **MLP (Multi-Layer Perceptron)**: A type of neural network with multiple hidden layers, effective for complex classification tasks.\n",
                "\n",
                "2. **k-NN (k-Nearest Neighbors)**: Simple for low-dimensional data but computationally heavy for large datasets.\n",
                "\n",
                "3. **Random Forest**: A popular method using multiple decision trees, effective for both classification and regression.\n",
                "\n",
                "4. **SVM (Support Vector Machine)**: Effective in high-dimensional spaces but can be slower than gradient boosting methods.\n",
                "\n",
                "5. **XGBoost (Extreme Gradient Boosting)**: Highly efficient and versatile, suitable for various supervised learning tasks.\n",
                "\n",
                "For <u>regression</u>, we're using the same methods as above, but replacing k-NN with **Linear Regression** - a simple yet effective model for regression tasks.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <u>Performance Metrics</u>\n",
                "- Define the metrics for evaluating model performance (e.g., accuracy, F1 score, ROC-AUC)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For our classification models, we use six metrics to evaluate performance:\n",
                "\n",
                "1. **Accuracy**: Ratio of correct predictions. Useful overall but can mislead in imbalanced datasets.\n",
                "\n",
                "2. **Precision**: Ratio of correct positive predictions. Vital when false positives are costly.\n",
                "\n",
                "3. **Recall**: Ratio of correct positives out of all actual positives. Key when false negatives are costly.\n",
                "\n",
                "4. **F1-Score**: Balances Precision and Recall. Used when both metrics are important.\n",
                "\n",
                "5. **AUC-ROC**: Indicates the model's ability to differentiate classes. Higher values are better.\n",
                "\n",
                "6. **Training Time**: Measures computational efficiency, important in scenarios with computational constraints.\n",
                "\n",
                "For our regression models, we use three metrics to evaluate performance:\n",
                "\n",
                "1. **Mean Absolute Error (MAE)**: Average of absolute errors. Useful for datasets with outliers.\n",
                "\n",
                "2. **Mean Squared Error (MSE)**: Average of squared errors. Useful for datasets without outliers.\n",
                "\n",
                "3. **Training Time**: Measures computational efficiency, important in scenarios with computational constraints."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <u>Dataset Description</u>\n",
                "- Introduce the dataset(s) used for the comparison.\n",
                "- Include feature descriptions and any preprocessing steps.\n",
                "\n",
                "#### Dataset 1: [The Titanic Dataset](https://www.kaggle.com/c/titanic/data) (famous for classification problems):\n",
                "| Attribute  | Description                                       | Key                                        |\n",
                "|-----------|-------------------------------------------------|--------------------------------------------|\n",
                "| PassengerId | Unique ID for each passenger                   |                                            |\n",
                "| Name      | Passenger name                                   |                                            |\n",
                "| pclass    | Ticket class                                     | 1 = 1st, 2 = 2nd, 3 = 3rd                  |\n",
                "| sex       | Sex                                              |                                            |\n",
                "| Age       | Age in years                                     |                                            |\n",
                "| sibsp     | # of siblings / spouses aboard the Titanic       |                                            |\n",
                "| parch     | # of parents / children aboard the Titanic       |                                            |\n",
                "| ticket    | Ticket number                                    |                                            |\n",
                "| fare      | Passenger fare                                   |                                            |\n",
                "| cabin     | Cabin number                                     |                                            |\n",
                "| embarked  | Port of Embarkation                              | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
                "| survived  | Survived or not                                         | 0 = No, 1 = Yes                            |\n",
                "\n",
                "\n",
                "#### Dataset 2: [The Boston Housing Dataset](https://www.kaggle.com/datasets/fedesoriano/the-boston-houseprice-data) (famous for regression problems):\n",
                "| Attribute | Description                                           | Unit        |\n",
                "|-----------|-------------------------------------------------------|-------------|\n",
                "| CRIM      | Per capita crime rate by town                         | -           |\n",
                "| ZN        | Proportion of residential land zoned for large lots   | -           |\n",
                "| INDUS     | Proportion of non-retail business acres per town      | -           |\n",
                "| CHAS      | Charles River dummy variable                          | 1 or 0      |\n",
                "| NOX       | Nitric oxides concentration                           | parts/10M   |\n",
                "| RM        | Average number of rooms per dwelling                  | -           |\n",
                "| AGE       | Proportion of owner-occupied units built pre-1940     | -           |\n",
                "| DIS       | Weighted distances to five Boston employment centres | -           |\n",
                "| RAD       | Index of accessibility to radial highways             | -           |\n",
                "| TAX       | Full-value property-tax rate                          | $/10k       |\n",
                "| PTRATIO   | Pupil-teacher ratio by town                           | -           |\n",
                "| B         | Equation result of black population proportion        | -           |\n",
                "| LSTAT     | Percentage of lower status population                 | %           |\n",
                "| MEDV      | Median value of owner-occupied homes                  | k$          |\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <u>Cross-Validation Strategy</u>\n",
                "- Explain the cross-validation process to ensure fairness in comparison.\n",
                "\n",
                "For the tuned models, we use 5-fold cross-validation to ensure fairness in comparison. For each model, we use the same cross-validation folds to train and evaluate the model. We also use the same random seed for each model to ensure that the folds are identical across models.\n",
                "\n",
                "For the untuned models, we use a single train-test split to train and evaluate the model. We use the same random seed for each model to ensure that the train-test split is identical across models."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <u>Hyperparameter Tuning</u>\n",
                "- How each model's hyperparameters are tuned for optimal performance.\n",
                "\n",
                "We use a grid search to tune the hyperparameters of each model with 5-fold cross-validation. The parameters and ranges used for each model are are chosen based on the model's documentation and best practices. The grid search is performed using the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) function from scikit-learn."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <u>Model Training and Evaluation</u>\n",
                "- Train the selected models on the dataset.\n",
                "- Evaluate and compare their performance using the defined metrics."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We are comparing both classification and regression.  For classification, we are using the Titanic dataset.  We fit to a simple reduced and feature mapped version of the dataset, as well as a complex heavily feature processed version of the dataset.\n",
                "\n",
                "For regression, we are using the Boston Housing dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# IMPORTS\n",
                "# Standard libraries\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import time\n",
                "\n",
                "# Sklearn modules\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.metrics import (accuracy_score, average_precision_score, classification_report, confusion_matrix, \n",
                "                             f1_score, precision_recall_curve, precision_score, recall_score, roc_auc_score, \n",
                "                             roc_curve, ConfusionMatrixDisplay, PrecisionRecallDisplay, RocCurveDisplay, mean_absolute_error, mean_squared_error, r2_score)\n",
                "from sklearn.model_selection import GridSearchCV, train_test_split\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.preprocessing import (FunctionTransformer, MinMaxScaler, OneHotEncoder, OrdinalEncoder, \n",
                "                                   PolynomialFeatures, StandardScaler)\n",
                "from sklearn.svm import SVC, SVR\n",
                "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
                "\n",
                "# Third-party libraries\n",
                "from xgboost import XGBClassifier, XGBRegressor\n",
                "\n",
                "# pd max cols\n",
                "pd.set_option('display.max_columns', 100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def titanic_data_transformed(t_data):\n",
                "\n",
                "    # Group ticket feature extraction\n",
                "    group_tickets = t_data['Ticket'].value_counts() > 1\n",
                "    t_data['Is_Group'] = t_data['Ticket'].apply(lambda x: 1 if group_tickets[x] else 0)\n",
                "    t_data['Group_Size'] = t_data['Ticket'].map(t_data['Ticket'].value_counts())\n",
                "\n",
                "    # Functions for title and family size extraction\n",
                "    def get_title(data):\n",
                "        titles = data['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
                "        common_titles = titles.value_counts().nlargest(6).index\n",
                "        return titles.where(titles.isin(common_titles), 'Other')\n",
                "\n",
                "    def get_family_size(data):\n",
                "        return data['SibSp'] + data['Parch'] + 1\n",
                "\n",
                "    # Transformation pipelines\n",
                "    numerical_features = ['Age', 'Fare', 'FamilySize', 'Group_Size']\n",
                "    numerical_transformer = Pipeline([\n",
                "        ('imputer', SimpleImputer(strategy='median')),\n",
                "        ('scaler', MinMaxScaler())\n",
                "    ])\n",
                "\n",
                "    categorical_features = ['Sex', 'Embarked', 'Title', 'Pclass']\n",
                "    categorical_transformer = Pipeline([\n",
                "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
                "    ])\n",
                "\n",
                "    # Apply the custom transformations\n",
                "    t_data['Title'] = get_title(t_data)\n",
                "    t_data['FamilySize'] = get_family_size(t_data)\n",
                "\n",
                "    # Column transformer\n",
                "    preprocessor = ColumnTransformer(\n",
                "        transformers=[\n",
                "            ('num', numerical_transformer, numerical_features),\n",
                "            ('cat', categorical_transformer, categorical_features)\n",
                "        ],\n",
                "        remainder='passthrough'\n",
                "    )\n",
                "\n",
                "    # Fit the transformer and transform the data\n",
                "    transformed_data = preprocessor.fit_transform(t_data)\n",
                "\n",
                "    # Getting the column names for numerical features\n",
                "    numerical_cols = numerical_features\n",
                "\n",
                "    # Getting the column names for categorical features\n",
                "    categorical_cols = preprocessor.named_transformers_['cat']['encoder'].get_feature_names_out(categorical_features)\n",
                "\n",
                "    # Combine all column names\n",
                "    all_cols = list(numerical_cols) + list(categorical_cols)\n",
                "\n",
                "    # Add any remaining columns that were 'passed through'\n",
                "    pass_through_cols = [col for col in t_data.columns if col not in numerical_features + categorical_features]\n",
                "    all_cols.extend(pass_through_cols)\n",
                "\n",
                "    # Create the DataFrame\n",
                "    transformed_df = pd.DataFrame(transformed_data, columns=all_cols)\n",
                "\n",
                "    transformed_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
                "\n",
                "    # Set all dtypes to float\n",
                "    transformed_df = transformed_df.astype(float)\n",
                "\n",
                "    return transformed_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Load Titanic data\n",
                "t_data = pd.read_csv('../../data_science_analytics_class/Data/titanic.csv')\n",
                "\n",
                "t_data_reduced = t_data.drop(columns=['Name', 'Ticket', 'Cabin'])\n",
                "titanic_data = t_data_reduced\n",
                "\n",
                "# map Embarked to numeric values and sex to numeric values\n",
                "titanic_data['Embarked'] = titanic_data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
                "titanic_data['Sex'] = titanic_data['Sex'].map({'male': 1, 'female': 0})\n",
                "\n",
                "# fill missing age with mean based on Pclass\n",
                "titanic_data['Age'] = titanic_data['Age'].fillna(titanic_data.groupby('Pclass')['Age'].transform('mean'))\n",
                "\n",
                "# drop missing embarked rows\n",
                "titanic_data.dropna(subset=['Embarked'], inplace=True)\n",
                "\n",
                "# Load transformed titanic data\n",
                "# titanic_data = titanic_data_transformed(t_data)\n",
                "\n",
                "# Load Boston housing data\n",
                "b_data = pd.read_csv('../../data_science_analytics_class/Data/boston_housing.csv')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### Classification \n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Default params:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\bovam\\AppData\\Local\\Temp\\ipykernel_10480\\2342740577.py:64: FutureWarning: this method is deprecated in favour of `Styler.format(precision=..)`\n",
                        "  styled_results_classification = results_df_transposed.style.apply(highlight_min_or_max).set_precision(3)\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "#T_9c9a1_row1_col4, #T_9c9a1_row4_col0, #T_9c9a1_row4_col1, #T_9c9a1_row4_col2, #T_9c9a1_row4_col3 {\n",
                            "  background-color: green;\n",
                            "}\n",
                            "</style>\n",
                            "<table id=\"T_9c9a1\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_9c9a1_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
                            "      <th id=\"T_9c9a1_level0_col1\" class=\"col_heading level0 col1\" >Precision</th>\n",
                            "      <th id=\"T_9c9a1_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
                            "      <th id=\"T_9c9a1_level0_col3\" class=\"col_heading level0 col3\" >F1-Score</th>\n",
                            "      <th id=\"T_9c9a1_level0_col4\" class=\"col_heading level0 col4\" >Training Time</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_9c9a1_level0_row0\" class=\"row_heading level0 row0\" >MLP</th>\n",
                            "      <td id=\"T_9c9a1_row0_col0\" class=\"data row0 col0\" >0.744</td>\n",
                            "      <td id=\"T_9c9a1_row0_col1\" class=\"data row0 col1\" >0.739</td>\n",
                            "      <td id=\"T_9c9a1_row0_col2\" class=\"data row0 col2\" >0.744</td>\n",
                            "      <td id=\"T_9c9a1_row0_col3\" class=\"data row0 col3\" >0.734</td>\n",
                            "      <td id=\"T_9c9a1_row0_col4\" class=\"data row0 col4\" >0.511</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_9c9a1_level0_row1\" class=\"row_heading level0 row1\" >k-NN</th>\n",
                            "      <td id=\"T_9c9a1_row1_col0\" class=\"data row1 col0\" >0.610</td>\n",
                            "      <td id=\"T_9c9a1_row1_col1\" class=\"data row1 col1\" >0.590</td>\n",
                            "      <td id=\"T_9c9a1_row1_col2\" class=\"data row1 col2\" >0.610</td>\n",
                            "      <td id=\"T_9c9a1_row1_col3\" class=\"data row1 col3\" >0.595</td>\n",
                            "      <td id=\"T_9c9a1_row1_col4\" class=\"data row1 col4\" >0.021</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_9c9a1_level0_row2\" class=\"row_heading level0 row2\" >Random Forest</th>\n",
                            "      <td id=\"T_9c9a1_row2_col0\" class=\"data row2 col0\" >0.798</td>\n",
                            "      <td id=\"T_9c9a1_row2_col1\" class=\"data row2 col1\" >0.797</td>\n",
                            "      <td id=\"T_9c9a1_row2_col2\" class=\"data row2 col2\" >0.798</td>\n",
                            "      <td id=\"T_9c9a1_row2_col3\" class=\"data row2 col3\" >0.797</td>\n",
                            "      <td id=\"T_9c9a1_row2_col4\" class=\"data row2 col4\" >0.289</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_9c9a1_level0_row3\" class=\"row_heading level0 row3\" >SVM</th>\n",
                            "      <td id=\"T_9c9a1_row3_col0\" class=\"data row3 col0\" >0.655</td>\n",
                            "      <td id=\"T_9c9a1_row3_col1\" class=\"data row3 col1\" >0.679</td>\n",
                            "      <td id=\"T_9c9a1_row3_col2\" class=\"data row3 col2\" >0.655</td>\n",
                            "      <td id=\"T_9c9a1_row3_col3\" class=\"data row3 col3\" >0.558</td>\n",
                            "      <td id=\"T_9c9a1_row3_col4\" class=\"data row3 col4\" >0.158</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_9c9a1_level0_row4\" class=\"row_heading level0 row4\" >XGBoost</th>\n",
                            "      <td id=\"T_9c9a1_row4_col0\" class=\"data row4 col0\" >0.803</td>\n",
                            "      <td id=\"T_9c9a1_row4_col1\" class=\"data row4 col1\" >0.800</td>\n",
                            "      <td id=\"T_9c9a1_row4_col2\" class=\"data row4 col2\" >0.803</td>\n",
                            "      <td id=\"T_9c9a1_row4_col3\" class=\"data row4 col3\" >0.800</td>\n",
                            "      <td id=\"T_9c9a1_row4_col4\" class=\"data row4 col4\" >0.085</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x298d5056490>"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# DEFAULT PARAMS ALL MODELS\n",
                "\n",
                "# Feature matrix X, target vector y\n",
                "X = titanic_data.drop(['Survived'], axis=1)\n",
                "y = titanic_data['Survived']\n",
                "\n",
                "\n",
                "models = {\n",
                "    \"MLP\": MLPClassifier(max_iter=1000),\n",
                "    \"k-NN\": KNeighborsClassifier(),\n",
                "    \"Random Forest\": RandomForestClassifier(),\n",
                "    \"SVM\": SVC(probability=True),\n",
                "    \"XGBoost\": XGBClassifier()\n",
                "}\n",
                "\n",
                "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
                "    start = time.time()\n",
                "    model.fit(X_train, y_train)\n",
                "    y_pred = model.predict(X_test)\n",
                "    y_score = model.predict_proba(X_test)\n",
                "    end = time.time()\n",
                "    training_time = end - start\n",
                "    scores = {\n",
                "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
                "        \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n",
                "        \"Recall\": recall_score(y_test, y_pred, average='weighted'),\n",
                "        \"F1-Score\": f1_score(y_test, y_pred, average='weighted'),\n",
                "        # \"AUC-ROC\": roc_auc_score(y_test, y_score, average='weighted'),\n",
                "        # \"AUC-PR\": average_precision_score(y_test, y_score, average='weighted'), # Not directly supported for multiclass\n",
                "        \"Training Time\": training_time\n",
                "    }\n",
                "    # Create plots to add to dictionary\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    cm_display_data = (cm, model.classes_)\n",
                "    \n",
                "    # Store the model and test set for ROC curve plotting\n",
                "    roc_data = (model, X_test, y_test)\n",
                "\n",
                "    plots = {\n",
                "        \"Confusion_Matrix\": cm_display_data,\n",
                "        \"ROC_Curve\": roc_data\n",
                "    }\n",
                "    return scores, plots\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
                "\n",
                "results = {}\n",
                "plots = {}\n",
                "for name, model in models.items():\n",
                "    results[name], plots[name] = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "def highlight_min_or_max(s):\n",
                "    if s.name in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:  # Adjust indices as needed\n",
                "        is_highlight = s == s.max()\n",
                "    elif s.name == 'Training Time':  # Adjust index as needed\n",
                "        is_highlight = s == s.min()\n",
                "    else:\n",
                "        is_highlight = [False] * len(s)  # No highlight for other columns\n",
                "    return ['background-color: green' if cell else '' for cell in is_highlight]\n",
                "# Transpose the DataFrame for better readability\n",
                "results_df_transposed = results_df.T\n",
                "# Apply the highlighting style\n",
                "styled_results_classification = results_df_transposed.style.apply(highlight_min_or_max).set_precision(3)\n",
                "styled_results_classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot ROC curves in a separate figure\n",
                "fig_roc, ax_roc = plt.subplots(figsize=(10, 8))\n",
                "for model_name in models.keys():\n",
                "    model, X_test, y_test = plots[model_name][\"ROC_Curve\"]\n",
                "    RocCurveDisplay.from_estimator(model, X_test, y_test, ax=ax_roc, name=model_name)\n",
                "ax_roc.set_title('ROC Curves')\n",
                "ax_roc.legend()\n",
                "# save the figure\n",
                "plt.savefig('../images/roc_curves_default.png')\n",
                "plt.show()\n",
                "\n",
                "# Plot confusion matrices in another separate figure\n",
                "fig_cm, axes_cm = plt.subplots(nrows=2, ncols=2, figsize=(10, 10)) # plotting 2x2, the first 4 models in models dict\n",
                "for ax, model_name in zip(axes_cm.flatten(), models.keys()):\n",
                "    cm, labels = plots[model_name][\"Confusion_Matrix\"]\n",
                "    ConfusionMatrixDisplay(cm, display_labels=labels).plot(ax=ax, cmap='Blues')\n",
                "    ax.set_title(model_name)\n",
                "# save the figure\n",
                "plt.savefig('../images/confusion_matrices_default_compare.png')\n",
                "\n",
                "fig_cm_xgb, ax_cm_xgb = plt.subplots(figsize=(5, 5))\n",
                "cm, labels = plots[\"XGBoost\"][\"Confusion_Matrix\"]\n",
                "ConfusionMatrixDisplay(cm, display_labels=labels).plot(ax=ax_cm_xgb, cmap='Blues')\n",
                "ax_cm_xgb.set_title(\"XGBoost\")\n",
                "# save the figure\n",
                "plt.savefig('../images/confusion_matrices_default_xgb.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Tuned params:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
                        "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
                        "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
                        "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
                        "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\bovam\\AppData\\Local\\Temp\\ipykernel_10480\\649164776.py:80: FutureWarning: this method is deprecated in favour of `Styler.format(precision=..)`\n",
                        "  styled_grid_results_classification = grid_results_df.style.apply(highlight_min_or_max).set_precision(3)\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<style type=\"text/css\">\n",
                            "</style>\n",
                            "<table id=\"T_514ab\">\n",
                            "  <thead>\n",
                            "    <tr>\n",
                            "      <th class=\"blank level0\" >&nbsp;</th>\n",
                            "      <th id=\"T_514ab_level0_col0\" class=\"col_heading level0 col0\" >Best Parameters</th>\n",
                            "      <th id=\"T_514ab_level0_col1\" class=\"col_heading level0 col1\" >Best Score</th>\n",
                            "      <th id=\"T_514ab_level0_col2\" class=\"col_heading level0 col2\" >Test Accuracy</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th id=\"T_514ab_level0_row0\" class=\"row_heading level0 row0\" >MLP</th>\n",
                            "      <td id=\"T_514ab_row0_col0\" class=\"data row0 col0\" >{'activation': 'tanh', 'hidden_layer_sizes': (50,), 'solver': 'adam'}</td>\n",
                            "      <td id=\"T_514ab_row0_col1\" class=\"data row0 col1\" >0.793</td>\n",
                            "      <td id=\"T_514ab_row0_col2\" class=\"data row0 col2\" >0.794</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_514ab_level0_row1\" class=\"row_heading level0 row1\" >k-NN</th>\n",
                            "      <td id=\"T_514ab_row1_col0\" class=\"data row1 col0\" >{'n_neighbors': 7, 'weights': 'distance'}</td>\n",
                            "      <td id=\"T_514ab_row1_col1\" class=\"data row1 col1\" >0.658</td>\n",
                            "      <td id=\"T_514ab_row1_col2\" class=\"data row1 col2\" >0.605</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_514ab_level0_row2\" class=\"row_heading level0 row2\" >Random Forest</th>\n",
                            "      <td id=\"T_514ab_row2_col0\" class=\"data row2 col0\" >{'max_depth': 10, 'n_estimators': 200}</td>\n",
                            "      <td id=\"T_514ab_row2_col1\" class=\"data row2 col1\" >0.833</td>\n",
                            "      <td id=\"T_514ab_row2_col2\" class=\"data row2 col2\" >0.807</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_514ab_level0_row3\" class=\"row_heading level0 row3\" >SVM</th>\n",
                            "      <td id=\"T_514ab_row3_col0\" class=\"data row3 col0\" >{'C': 10, 'kernel': 'linear'}</td>\n",
                            "      <td id=\"T_514ab_row3_col1\" class=\"data row3 col1\" >0.814</td>\n",
                            "      <td id=\"T_514ab_row3_col2\" class=\"data row3 col2\" >0.830</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th id=\"T_514ab_level0_row4\" class=\"row_heading level0 row4\" >XGBoost</th>\n",
                            "      <td id=\"T_514ab_row4_col0\" class=\"data row4 col0\" >{'learning_rate': 0.01, 'n_estimators': 100}</td>\n",
                            "      <td id=\"T_514ab_row4_col1\" class=\"data row4 col1\" >0.818</td>\n",
                            "      <td id=\"T_514ab_row4_col2\" class=\"data row4 col2\" >0.812</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<pandas.io.formats.style.Styler at 0x298cf225490>"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# TUNED PARAMS ALL MODELS\n",
                "\n",
                "# Feature matrix X, target vector y\n",
                "X = titanic_data.drop(['Survived'], axis=1)\n",
                "y = titanic_data['Survived']\n",
                "\n",
                "# Define the models and their corresponding parameter grids\n",
                "model_params = {\n",
                "    \"MLP\": {\n",
                "        \"model\": MLPClassifier(max_iter=10000),\n",
                "        \"params\": {\n",
                "            \"hidden_layer_sizes\": [(50,), (100,)],\n",
                "            \"activation\": [\"relu\", \"tanh\"],\n",
                "            \"solver\": [\"adam\", \"sgd\"]\n",
                "        }\n",
                "    },\n",
                "    \"k-NN\": {\n",
                "        \"model\": KNeighborsClassifier(),\n",
                "        \"params\": {\n",
                "            \"n_neighbors\": [3, 5, 7],\n",
                "            \"weights\": [\"uniform\", \"distance\"]\n",
                "        }\n",
                "    },\n",
                "    \"Random Forest\": {\n",
                "        \"model\": RandomForestClassifier(),\n",
                "        \"params\": {\n",
                "            \"n_estimators\": [100, 200],\n",
                "            \"max_depth\": [None, 10, 20]\n",
                "        }\n",
                "    },\n",
                "    \"SVM\": {\n",
                "        \"model\": SVC(probability=True),\n",
                "        \"params\": {\n",
                "            \"C\": [0.1, 1, 10],\n",
                "            \"kernel\": [\"linear\", \"rbf\"]\n",
                "        }\n",
                "    },\n",
                "    \"XGBoost\": {\n",
                "        \"model\": XGBClassifier(),\n",
                "        \"params\": {\n",
                "            \"n_estimators\": [100, 200],\n",
                "            \"learning_rate\": [0.01, 0.1]\n",
                "        }\n",
                "    }\n",
                "}\n",
                "\n",
                "# Split your data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
                "\n",
                "# Function to perform grid search and evaluate each model\n",
                "def grid_search_model(model_info, X_train, X_test, y_train, y_test):\n",
                "    grid_search = GridSearchCV(model_info['model'], model_info['params'], cv=5, n_jobs=-1, verbose=3)\n",
                "    grid_search.fit(X_train, y_train)\n",
                "    best_model = grid_search.best_estimator_\n",
                "    y_pred = best_model.predict(X_test)\n",
                "    scores = {\n",
                "        \"Best Parameters\": grid_search.best_params_,\n",
                "        \"Best Score\": grid_search.best_score_,\n",
                "        \"Test Accuracy\": accuracy_score(y_test, y_pred)\n",
                "        # Add other metrics as needed\n",
                "    }\n",
                "    return scores\n",
                "\n",
                "def highlight_min_or_max(s):\n",
                "    if s.name in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:  # Adjust indices as needed\n",
                "        is_highlight = s == s.max()\n",
                "    elif s.name == 'Training Time':  # Adjust index as needed\n",
                "        is_highlight = s == s.min()\n",
                "    else:\n",
                "        is_highlight = [False] * len(s)  # No highlight for other columns\n",
                "    return ['background-color: green' if cell else '' for cell in is_highlight]\n",
                "\n",
                "# Evaluate each model\n",
                "grid_results = {}\n",
                "for name, model_info in model_params.items():\n",
                "    grid_results[name] = grid_search_model(model_info, X_train, X_test, y_train, y_test)\n",
                "\n",
                "# Convert results to DataFrame for display\n",
                "grid_results_df = pd.DataFrame(grid_results).T\n",
                "styled_grid_results_classification = grid_results_df.style.apply(highlight_min_or_max).set_precision(3)\n",
                "styled_grid_results_classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'Best Parameters': {'learning_rate': 0.01, 'n_estimators': 250},\n",
                            " 'Best Score': 0.8243182583323982,\n",
                            " 'Test Accuracy': 0.8340807174887892}"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "xgboosta = XGBClassifier()\n",
                "param_dict = {\n",
                "    \"n_estimators\": [100, 150, 200, 250, 300],\n",
                "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
                "    # \"max_depth\": [2, 5, 10],\n",
                "    # \"min_child_weight\": [1, 2, 4, 8, 16],\n",
                "    # \"alpha\": [0, 0.1, 0.5, 1],\n",
                "    # \"lambda\": [0, 0.1, 0.5, 1],\n",
                "    # \"gamma\": [0, 0.1, 0.5, 1],\n",
                "    # \"subsample\": [0.5, 0.75, 1],\n",
                "}\n",
                "\n",
                "grid_search = GridSearchCV(xgboosta, param_dict, cv=5, n_jobs=-1, verbose=3)\n",
                "grid_search.fit(X_train, y_train)\n",
                "best_model = grid_search.best_estimator_\n",
                "y_pred = best_model.predict(X_test)\n",
                "scores = {\n",
                "    \"Best Parameters\": grid_search.best_params_,\n",
                "    \"Best Score\": grid_search.best_score_,\n",
                "    \"Test Accuracy\": accuracy_score(y_test, y_pred)\n",
                "    # Add other metrics as needed\n",
                "}\n",
                "scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot ROC curves in a separate figure\n",
                "fig_roc, ax_roc = plt.subplots(figsize=(10, 8))\n",
                "for model_name in models.keys():\n",
                "    model, X_test, y_test = plots[model_name][\"ROC_Curve\"]\n",
                "    RocCurveDisplay.from_estimator(model, X_test, y_test, ax=ax_roc, name=model_name)\n",
                "ax_roc.set_title('ROC Curves')\n",
                "ax_roc.legend()\n",
                "# save the figure\n",
                "plt.savefig('../images/roc_curves_tuned.png')\n",
                "plt.show()\n",
                "\n",
                "# Plot confusion matrices in another separate figure\n",
                "fig_cm, axes_cm = plt.subplots(nrows=2, ncols=2, figsize=(10, 10)) # plotting 2x2, the first 4 models in models dict\n",
                "for ax, model_name in zip(axes_cm.flatten(), models.keys()):\n",
                "    cm, labels = plots[model_name][\"Confusion_Matrix\"]\n",
                "    ConfusionMatrixDisplay(cm, display_labels=labels).plot(ax=ax, cmap='Blues')\n",
                "    ax.set_title(model_name)\n",
                "# save the figure\n",
                "plt.savefig('../images/confusion_matrices_tuned_compare.png')\n",
                "\n",
                "fig_cm_xgb, ax_cm_xgb = plt.subplots(figsize=(5, 5))\n",
                "cm, labels = plots[\"XGBoost\"][\"Confusion_Matrix\"]\n",
                "ConfusionMatrixDisplay(cm, display_labels=labels).plot(ax=ax_cm_xgb, cmap='Blues')\n",
                "ax_cm_xgb.set_title(\"XGBoost\")\n",
                "# save the figure\n",
                "plt.savefig('../images/confusion_matrices_tuned_xgb.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### Regression\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Default params:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature matrix X, target vector y\n",
                "X = b_data.drop(['MEDV'], axis=1)\n",
                "y = b_data['MEDV']\n",
                "\n",
                "models = {\n",
                "    \"MLP\": MLPRegressor(max_iter=10000),\n",
                "    \"LinearRegression\": LinearRegression(),\n",
                "    \"Random Forest\": RandomForestRegressor(),\n",
                "    \"SVR\": SVR(),\n",
                "    \"XGBoost\": XGBRegressor()\n",
                "}\n",
                "\n",
                "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
                "    start = time.time()\n",
                "    model.fit(X_train, y_train)\n",
                "    y_pred = model.predict(X_test)\n",
                "    end = time.time()\n",
                "    training_time = end - start\n",
                "    scores = {\n",
                "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
                "        \"MSE\": mean_squared_error(y_test, y_pred),\n",
                "        \"R^2\": r2_score(y_test, y_pred),\n",
                "        \"Training Time\": training_time\n",
                "    }\n",
                "    return scores\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
                "\n",
                "results = {}\n",
                "plots = {}\n",
                "for name, model in models.items():\n",
                "    results[name] = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
                "    # Add info for plots (actual vs predicted)\n",
                "    model.fit(X_train, y_train)\n",
                "    y_pred = model.predict(X_test)\n",
                "    plots[name] = (y_test, y_pred)\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "def highlight_min_or_max(s):\n",
                "    if s.name in ['MAE', 'MSE', 'Training Time']:  # Adjust indices as needed\n",
                "        is_highlight = s == s.min()\n",
                "    elif s.name == 'R^2':  # Adjust index as needed\n",
                "        is_highlight = s == s.max()\n",
                "    else:\n",
                "        is_highlight = [False] * len(s)  # No highlight for other columns\n",
                "    return ['background-color: green' if cell else '' for cell in is_highlight]\n",
                "\n",
                "# Transpose the DataFrame for better readability\n",
                "results_df_transposed = results_df.T\n",
                "\n",
                "# Apply the highlighting style and set precision\n",
                "styled_results_regression = results_df_transposed.style.apply(highlight_min_or_max).format(precision=3)\n",
                "styled_results_regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot actual vs predicted in a separate figure\n",
                "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))  # plotting 2x2, the first 4 models in models dict\n",
                "for ax, model_name in zip(axes.flatten(), models.keys()):\n",
                "    y_test, y_pred = plots[model_name]\n",
                "    ax.scatter(y_test, y_pred)\n",
                "    ax.set_title(model_name)\n",
                "    ax.set_xlabel('Actual')\n",
                "    ax.set_ylabel('Predicted')\n",
                "    # include metrics\n",
                "    mae = mean_absolute_error(y_test, y_pred)\n",
                "    mse = mean_squared_error(y_test, y_pred)\n",
                "    r2 = r2_score(y_test, y_pred)\n",
                "    ax.text(0.05, 0.85, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nR^2: {r2:.2f}', transform=ax.transAxes)\n",
                "    # add a line for perfect correlation\n",
                "    ax.plot([0, 50], [0, 50], color='red')\n",
                "# save the figure\n",
                "plt.savefig('../images/actual_vs_predicted_compare.png')\n",
                "\n",
                "fig_xgb, ax_xgb = plt.subplots(figsize=(5, 5))\n",
                "y_test, y_pred = plots[\"XGBoost\"]\n",
                "ax_xgb.scatter(y_test, y_pred)\n",
                "ax_xgb.set_title(\"XGBoost\")\n",
                "ax_xgb.set_xlabel('Actual')\n",
                "ax_xgb.set_ylabel('Predicted')\n",
                "# include metrics\n",
                "mae = mean_absolute_error(y_test, y_pred)\n",
                "mse = mean_squared_error(y_test, y_pred)\n",
                "r2 = r2_score(y_test, y_pred)\n",
                "ax_xgb.text(0.05, 0.85, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nR^2: {r2:.2f}', transform=ax_xgb.transAxes)\n",
                "# add a line for perfect correlation\n",
                "ax_xgb.plot([0, 50], [0, 50], color='red')\n",
                "# save the figure\n",
                "plt.savefig('../images/actual_vs_predicted_xgb.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Tuned params:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature matrix X, target vector y\n",
                "X = b_data.drop(['MEDV'], axis=1)\n",
                "y = b_data['MEDV']\n",
                "\n",
                "# Define the models and their corresponding parameter grids\n",
                "model_params = {\n",
                "    \"MLP\": {\n",
                "        \"model\": MLPRegressor(max_iter=10000),\n",
                "        \"params\": {\n",
                "            \"hidden_layer_sizes\": [(50,), (100,)],\n",
                "            \"activation\": [\"relu\", \"tanh\"],\n",
                "            \"solver\": [\"adam\", \"sgd\"]\n",
                "        }\n",
                "    },\n",
                "    \"Linear Regression\": {\n",
                "        \"model\": LinearRegression(),\n",
                "        \"params\": {\n",
                "            \"fit_intercept\": [True, False],\n",
                "        }\n",
                "    },\n",
                "    \"Random Forest\": {\n",
                "        \"model\": RandomForestRegressor(),\n",
                "        \"params\": {\n",
                "            \"n_estimators\": [100, 200],\n",
                "            \"max_depth\": [None, 10, 20]\n",
                "        }\n",
                "    },\n",
                "    \"SVM\": {\n",
                "        \"model\": SVR(),\n",
                "        \"params\": {\n",
                "            \"C\": [0.1, 1, 10],\n",
                "            \"kernel\": [\"linear\", \"rbf\"]\n",
                "        }\n",
                "    },\n",
                "    \"XGBoost\": {\n",
                "        \"model\": XGBRegressor(),\n",
                "        \"params\": {\n",
                "            \"n_estimators\": [100, 200],\n",
                "            \"learning_rate\": [0.01, 0.1]\n",
                "        }\n",
                "    }\n",
                "}\n",
                "\n",
                "# Split your data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
                "\n",
                "# Function to perform grid search and evaluate each model\n",
                "def grid_search_model(model_info, X_train, X_test, y_train, y_test):\n",
                "    grid_search = GridSearchCV(model_info['model'], model_info['params'], cv=5, n_jobs=-1, verbose=1)\n",
                "    grid_search.fit(X_train, y_train)\n",
                "    best_model = grid_search.best_estimator_\n",
                "    y_pred = best_model.predict(X_test)\n",
                "    scores = {\n",
                "        \"Best Parameters\": grid_search.best_params_,\n",
                "        \"Best Score\": grid_search.best_score_,\n",
                "        \"Test MAE\": mean_absolute_error(y_test, y_pred),\n",
                "        \"Test MSE\": mean_squared_error(y_test, y_pred),\n",
                "        \"Test R^2\": r2_score(y_test, y_pred)\n",
                "        # Add other metrics as needed\n",
                "    }\n",
                "    return scores\n",
                "\n",
                "def highlight_min_or_max(s):\n",
                "    if s.name in ['MAE', 'MSE', 'Training Time']:  # Adjust indices as needed\n",
                "        is_highlight = s == s.min()\n",
                "    elif s.name == 'R^2':  # Adjust index as needed\n",
                "        is_highlight = s == s.max()\n",
                "    else:\n",
                "        is_highlight = [False] * len(s)  # No highlight for other columns\n",
                "    return ['background-color: green' if cell else '' for cell in is_highlight]\n",
                "\n",
                "\n",
                "# Evaluate each model\n",
                "grid_results = {}\n",
                "for name, model_info in model_params.items():\n",
                "    grid_results[name] = grid_search_model(model_info, X_train, X_test, y_train, y_test)\n",
                "\n",
                "# Convert results to DataFrame for display\n",
                "grid_results_df = pd.DataFrame(grid_results).T\n",
                "styled_grid_results_regression = grid_results_df.style.apply(highlight_min_or_max).set_precision(3)\n",
                "styled_grid_results_regression"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <u>Result Analysis</u>\n",
                "- Present the comparison results in tables or graphs.\n",
                "- Statistical tests, if applicable, to establish significant differences."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <u>Discussion</u>\n",
                "- Interpret the comparison findings.\n",
                "- Discuss where XGBoost outperforms or underperforms."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <u>Conclusion</u>\n",
                "- Summarize key takeaways from the XGBoost exploration and model comparison."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Appendices and Supporting Materials\n",
                "\n",
                "- Code snippets, Jupyter Notebook links, or GitHub repository.\n",
                "- Detailed tables and graphical representations of results.\n",
                "- Additional notes on the computational environment, data access, etc."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### References:"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[1] Introduction to boosted trees, Introduction to Boosted Trees - xgboost 2.0.2 documentation, https://xgboost.readthedocs.io/en/stable/tutorials/model.html (accessed Nov. 28, 2023). \n",
                "\n",
                "[2] C. Wade, Hands-on Gradient Boosting with XGBoost and Scikit-Learn: Perform Accessible Machine Learning and Extreme Gradient Boosting with Python. Birmingham: Packt Publishing, 2020. \n",
                "\n",
                "[3] G. James, D. Witten, T. Hastie, R. Tibshirani, and J. Taylor, 8.2: Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees, in An introduction to statistical learning with applications in Python, Cham: Springer International Publishing, 2023, pp. 343354 \n",
                "\n",
                "[4] Frequently asked questions, Frequently Asked Questions - xgboost 2.0.2 documentation, https://xgboost.readthedocs.io/en/stable/faq.html (accessed Nov. 28, 2023). \n",
                "\n",
                "[5] XGBoost parameters, XGBoost Parameters - xgboost 2.0.2 documentation, https://xgboost.readthedocs.io/en/stable/parameter.html (accessed Nov. 28, 2023). \n",
                "\n",
                "[] XGBoost documentation, XGBoost Documentation - xgboost 2.1.0-dev documentation, https://xgboost.readthedocs.io/en/latest/ (accessed Nov. 28, 2023). \n",
                "\n",
                "[] Stuarthallows, Using XGBoost with Scikit-Learn, Kaggle, https://www.kaggle.com/code/stuarthallows/using-xgboost-with-scikit-learn (accessed Nov. 28, 2023). \n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "sportsenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

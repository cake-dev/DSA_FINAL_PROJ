{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Benchmarking XGBoost Against Other Machine Learning Models\n",
    "\n",
    "---\n",
    "\n",
    "## Part I: Understanding XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "- Briefly introduce machine learning and the role of ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background on Boosting\n",
    "- Explain the concept of boosting in machine learning.\n",
    "- Historical evolution leading to gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Overview\n",
    "- Detailed explanation of XGBoost and its core algorithm.\n",
    "- Advantages of XGBoost over other boosting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts and Features of XGBoost\n",
    "- Discuss tree boosting, regularized learning, and model complexity.\n",
    "- Overview of handling missing data, parallel processing, and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Parameters\n",
    "- List and explain crucial XGBoost hyperparameters.\n",
    "- Show how these parameters can affect model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation and Setup\n",
    "- Guide on setting up XGBoost in a development environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "- Discuss the preprocessing required for optimal XGBoost performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with XGBoost\n",
    "- Step-by-step process of training an XGBoost model.\n",
    "- Techniques for evaluating model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "- How to interpret model outputs, importance scores, and diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Performance Comparison of XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Benchmarking Goals</u>\n",
    "- Define the objectives of the performance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of the performance comparison are to:\n",
    "- Compare the performance of XGBoost against other machine learning models.\n",
    "- Determine the optimal hyperparameters for each model.\n",
    "- Identify the best model for the given dataset (may or may not be XGBoost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Selection of Competing Models</u>\n",
    "- Choose a set of models for comparison (e.g., Random Forest, SVM, Neural Networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using six machine learning models for comparison. Here is a brief description of each:\n",
    "\n",
    "1. **MLP (Multi-Layer Perceptron)**: A type of neural network with multiple hidden layers, effective for complex classification tasks.\n",
    "\n",
    "2. **GradientBoosting**: A simpler gradient boosting classifier with fewer hyperparameters, ideal as a starting point. For advanced optimization, XGBoost or LightGBM are preferred.\n",
    "\n",
    "3. **k-NN (k-Nearest Neighbors)**: Simple for low-dimensional data but computationally heavy for large datasets.\n",
    "\n",
    "4. **Random Forest**: A popular method using multiple decision trees, effective for both classification and regression.\n",
    "\n",
    "5. **SVM (Support Vector Machine)**: Effective in high-dimensional spaces but can be slower than gradient boosting methods.\n",
    "\n",
    "6. **XGBoost (Extreme Gradient Boosting)**: Highly efficient and versatile, suitable for various supervised learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Dataset Description</u>\n",
    "- Introduce the dataset(s) used for the comparison.\n",
    "- Include feature descriptions and any preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Performance Metrics</u>\n",
    "- Define the metrics for evaluating model performance (e.g., accuracy, F1 score, ROC-AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our classification models, we use seven metrics to evaluate performance:\n",
    "\n",
    "1. **Accuracy**: Ratio of correct predictions. Useful overall but can mislead in imbalanced datasets.\n",
    "\n",
    "2. **Precision**: Ratio of correct positive predictions. Vital when false positives are costly.\n",
    "\n",
    "3. **Recall (Sensitivity)**: Ratio of correct positives out of all actual positives. Key when false negatives are costly.\n",
    "\n",
    "4. **F1-Score**: Balances Precision and Recall. Used when both metrics are important.\n",
    "\n",
    "5. **AUC-ROC**: Indicates the model's ability to differentiate classes. Higher values are better.\n",
    "\n",
    "6. **AUC-PR**: Focuses on performance regarding the positive class, crucial in imbalanced datasets.\n",
    "\n",
    "7. **Training Time**: Measures computational efficiency, important in scenarios with computational constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Cross-Validation Strategy</u>\n",
    "- Explain the cross-validation process to ensure fairness in comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Hyperparameter Tuning</u>\n",
    "- How each model's hyperparameters are tuned for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Model Training and Evaluation</u>\n",
    "- Train the selected models on the dataset.\n",
    "- Evaluate and compare their performance using the defined metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using default params for all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jake/miniconda3/envs/sportsenv/lib/python3.10/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/home/jake/miniconda3/envs/sportsenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLP</th>\n",
       "      <th>GradientBoosting</th>\n",
       "      <th>k-NN</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>SVM</th>\n",
       "      <th>XGBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.331441</td>\n",
       "      <td>0.946296</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.801058</td>\n",
       "      <td>0.974074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-Score</th>\n",
       "      <td>0.395286</td>\n",
       "      <td>0.943997</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.802427</td>\n",
       "      <td>0.971775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC-ROC</th>\n",
       "      <td>0.903770</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.894571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915584</td>\n",
       "      <td>0.998737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training Time</th>\n",
       "      <td>0.029195</td>\n",
       "      <td>0.391077</td>\n",
       "      <td>0.004426</td>\n",
       "      <td>0.128326</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.029552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    MLP  GradientBoosting      k-NN  Random Forest       SVM  \\\n",
       "Accuracy       0.500000          0.944444  0.722222       1.000000  0.805556   \n",
       "Precision      0.331441          0.946296  0.722222       1.000000  0.801058   \n",
       "Recall         0.500000          0.944444  0.722222       1.000000  0.805556   \n",
       "F1-Score       0.395286          0.943997  0.722222       1.000000  0.802427   \n",
       "AUC-ROC        0.903770          0.989899  0.894571       1.000000  0.915584   \n",
       "Training Time  0.029195          0.391077  0.004426       0.128326  0.004816   \n",
       "\n",
       "                XGBoost  \n",
       "Accuracy       0.972222  \n",
       "Precision      0.974074  \n",
       "Recall         0.972222  \n",
       "F1-Score       0.971775  \n",
       "AUC-ROC        0.998737  \n",
       "Training Time  0.029552  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"MLP\": MLPClassifier(max_iter=1000),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "    \"k-NN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    scores = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "        \"Recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "        \"F1-Score\": f1_score(y_test, y_pred, average='weighted'),\n",
    "        \"AUC-ROC\": roc_auc_score(y_test, y_score, multi_class='ovr', average='weighted'),\n",
    "        # \"AUC-PR\": average_precision_score(y_test, y_score, average='weighted'), # Not directly supported for multiclass\n",
    "        \"Training Time\": training_time\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    results[name] = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Result Analysis</u>\n",
    "- Present the comparison results in tables or graphs.\n",
    "- Statistical tests, if applicable, to establish significant differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Discussion</u>\n",
    "- Interpret the comparison findings.\n",
    "- Discuss where XGBoost outperforms or underperforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Conclusion</u>\n",
    "- Summarize key takeaways from the XGBoost exploration and model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices and Supporting Materials\n",
    "\n",
    "- Code snippets, Jupyter Notebook links, or GitHub repository.\n",
    "- Detailed tables and graphical representations of results.\n",
    "- Additional notes on the computational environment, data access, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- https://xgboost.readthedocs.io/en/latest/\n",
    "- https://www.kaggle.com/code/stuarthallows/using-xgboost-with-scikit-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sportsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

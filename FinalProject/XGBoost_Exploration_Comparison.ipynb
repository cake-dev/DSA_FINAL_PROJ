{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Benchmarking XGBoost Against Other Machine Learning Models\n",
    "\n",
    "---\n",
    "\n",
    "## Part I: Understanding XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "- Briefly introduce machine learning and the role of ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background on Boosting\n",
    "- Explain the concept of boosting in machine learning. Refresh the concept of boosing with an image depiction?\n",
    "    - Boosting is an ensemble method that trains sequential models on the errors, or residuals, of previous models, thus creating a strong learner via iteration on many weak learners. **ISL**  **& BOOK 92**\n",
    "\n",
    "- Historical evolution leading to gradient boosting.\n",
    "    - Boositng and other ensemble methods were popularized due standard decision trees overfitting data. Boosting allowed for a more generalizable model with \"consistency and power\". **BOOK 110**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Overview\n",
    "- Detailed explanation of XGBoost and its core algorithm.\n",
    "    - XGBoost, which stands for \"Extreme Gradient Boosting\", is an open source software who's algorithm uses gradient boosting to . Averages or the majority class are considered when outputing a result. **Read the docs: tutorials**\n",
    "\n",
    "- Advantages of XGBoost over other boosting methods.\n",
    "    - XGBoost is extremely fast, memory efficient, and can perform advance model operations **Read the docs: faq** **& Book 57**\n",
    "    - Not only is XGBoost fast, but it can provide better accuracy than other gradient boosting models. Speed can be attributed to it's ability to handle missing values, it storing sparce matrices of data, computing in parallel with blocks, being cache-aware. **Book 129**  \n",
    "        - missing values?????\n",
    "        -  from original XGBoost papaer: A Scalable Tree Boosting System, the sparsity-aware split-finding algorithm performed 50 times faster than the standard approach on the All-State-10K dataset.**Book 112**\n",
    "        - Parallel computing occurs when multiple computational units are working together on the same problem at the same time. XGBoost sorts and compresses the data into blocks. These blocks may be distributed to multiple machines, or to external memory (out of core). Sorting the data is faster with blocks. The split-finding algorithm takes advantage of blocks and the search for quantiles is faster due to blocks. In each of these cases, XGBoost provides parallel computing to expedite the model-building process. **Book 112**\n",
    "        - According to XGBoost: A Scalable Tree Boosting System, prefetching lengthens read/write dependency and reduces runtimes by approximately 50% for datasets with a large number of rows. **BOOK 112**\n",
    "    - And accuracy can be attributed to XGBoost's built-in regularization. \n",
    "        - Regularization is the process of adding information to reduce variance and prevent overfitting. **BOOK 112**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts and Features of XGBoost\n",
    "- Discuss tree boosting, regularized learning, and model complexity.\n",
    "- Overview of handling missing data, parallel processing, and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Parameters\n",
    "- List and explain crucial XGBoost hyperparameters.\n",
    "    -  XGBoost is easy to use with scikit-learn since \"all standard methods are available\" and scikit-learn's functions like train_test_split, and GridSearchCV can be combined with XGBoost models. **BOOK 117**\n",
    "    - Some hyperparameters are required to build a specific model. \n",
    "        - General parameters determine what booster we will be using. **Read the docs: parameter**\n",
    "        - Booster parameters will depend on the general parameter chosen. **Read the docs: parameter**\n",
    "        - Learning task parameters determine the \"objective\", or scoring metric, and the evaluation metric. **Read the docs: parameter**\n",
    "    - These hyperparameters \"are designed to improve upon accuracy and speed. **Book 130**\n",
    "\n",
    "    - Hyperparameters of specific models are similar to scikit-learn's hyperparameters:\n",
    "\n",
    "<img src=\"../images/XGBoost_hyperparameters.png\" alt=\"Some of XGBoost's core hyperparameters\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "- Show how these parameters can affect model performance. **Code for this?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation and Setup\n",
    "- ~~Guide on setting up XGBoost in a development environment.~~ May take up too much time. Perhaps link the documentation page on how to install and discuss to install via pip or conda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "- Discuss the preprocessing required for optimal XGBoost performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training with XGBoost\n",
    "- Step-by-step process of training an XGBoost model.\n",
    "- Techniques for evaluating model performance. \n",
    "\n",
    "- **Do this in the code block above where we're demo-ing parameters and hyperparameters**\n",
    "    - page 119 for step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "- How to interpret model outputs, importance scores, and diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Performance Comparison of XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Benchmarking Goals</u>\n",
    "- Define the objectives of the performance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of the performance comparison are to:\n",
    "- Compare the performance of XGBoost against other machine learning models.\n",
    "- Determine the optimal hyperparameters for each model.\n",
    "- Identify the best model for the given dataset (may or may not be XGBoost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Selection of Competing Models</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For <u>classification</u>, we're using six machine learning models for comparison. Here is a brief description of each:\n",
    "\n",
    "1. **MLP (Multi-Layer Perceptron)**: A type of neural network with multiple hidden layers, effective for complex classification tasks.\n",
    "\n",
    "2. **GradientBoosting**: A simpler gradient boosting classifier with fewer hyperparameters, ideal as a starting point.\n",
    "\n",
    "3. **k-NN (k-Nearest Neighbors)**: Simple for low-dimensional data but computationally heavy for large datasets.\n",
    "\n",
    "4. **Random Forest**: A popular method using multiple decision trees, effective for both classification and regression.\n",
    "\n",
    "5. **SVM (Support Vector Machine)**: Effective in high-dimensional spaces but can be slower than gradient boosting methods.\n",
    "\n",
    "6. **XGBoost (Extreme Gradient Boosting)**: Highly efficient and versatile, suitable for various supervised learning tasks.\n",
    "\n",
    "For <u>regression</u>, we're using the same methods as above, but replacing k-NN with **Linear Regression** - a simple yet effective model for regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Dataset Description</u>\n",
    "- Introduce the dataset(s) used for the comparison.\n",
    "- Include feature descriptions and any preprocessing steps.\n",
    "\n",
    "#### Dataset 1: [The Titanic Dataset](https://www.kaggle.com/c/titanic/data) (famous for classification problems)\n",
    "- The Titanic dataset is a classic dataset for classification problems. It contains 891 rows and 12 columns, with each row representing a passenger on the Titanic. The goal is to predict whether a passenger survived or not based on the given features. We will be using both a reduced simple version of the dataset and a fully transformed version of the dataset for comparison.\n",
    "\n",
    "#### Dataset 2: [The Boston Housing Dataset](https://www.kaggle.com/datasets/fedesoriano/the-boston-houseprice-data) (famous for regression problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Performance Metrics</u>\n",
    "- Define the metrics for evaluating model performance (e.g., accuracy, F1 score, ROC-AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our classification models, we use six metrics to evaluate performance:\n",
    "\n",
    "1. **Accuracy**: Ratio of correct predictions. Useful overall but can mislead in imbalanced datasets.\n",
    "\n",
    "2. **Precision**: Ratio of correct positive predictions. Vital when false positives are costly.\n",
    "\n",
    "3. **Recall**: Ratio of correct positives out of all actual positives. Key when false negatives are costly.\n",
    "\n",
    "4. **F1-Score**: Balances Precision and Recall. Used when both metrics are important.\n",
    "\n",
    "5. **AUC-ROC**: Indicates the model's ability to differentiate classes. Higher values are better.\n",
    "\n",
    "6. **Training Time**: Measures computational efficiency, important in scenarios with computational constraints.\n",
    "\n",
    "For our regression models, we use three metrics to evaluate performance:\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**: Average of absolute errors. Useful for datasets with outliers.\n",
    "\n",
    "2. **Mean Squared Error (MSE)**: Average of squared errors. Useful for datasets without outliers.\n",
    "\n",
    "3. **Training Time**: Measures computational efficiency, important in scenarios with computational constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Cross-Validation Strategy</u>\n",
    "- Explain the cross-validation process to ensure fairness in comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Hyperparameter Tuning</u>\n",
    "- How each model's hyperparameters are tuned for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Model Training and Evaluation</u>\n",
    "- Train the selected models on the dataset.\n",
    "- Evaluate and compare their performance using the defined metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using default params for all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Sklearn modules\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, average_precision_score, classification_report, confusion_matrix, \n",
    "                             f1_score, precision_recall_curve, precision_score, recall_score, roc_auc_score, \n",
    "                             roc_curve, ConfusionMatrixDisplay, PrecisionRecallDisplay, RocCurveDisplay)\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsClassifier as KNN\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (FunctionTransformer, MinMaxScaler, OneHotEncoder, OrdinalEncoder, \n",
    "                                   PolynomialFeatures, StandardScaler)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Third-party libraries\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# pd max cols\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titanic_data_transformed(t_data):\n",
    "\n",
    "    # Group ticket feature extraction\n",
    "    group_tickets = t_data['Ticket'].value_counts() > 1\n",
    "    t_data['Is_Group'] = t_data['Ticket'].apply(lambda x: 1 if group_tickets[x] else 0)\n",
    "    t_data['Group_Size'] = t_data['Ticket'].map(t_data['Ticket'].value_counts())\n",
    "\n",
    "    # Functions for title and family size extraction\n",
    "    def get_title(data):\n",
    "        titles = data['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
    "        common_titles = titles.value_counts().nlargest(6).index\n",
    "        return titles.where(titles.isin(common_titles), 'Other')\n",
    "\n",
    "    def get_family_size(data):\n",
    "        return data['SibSp'] + data['Parch'] + 1\n",
    "\n",
    "    # Transformation pipelines\n",
    "    numerical_features = ['Age', 'Fare', 'FamilySize', 'Group_Size']\n",
    "    numerical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_features = ['Sex', 'Embarked', 'Title', 'Pclass']\n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Apply the custom transformations\n",
    "    t_data['Title'] = get_title(t_data)\n",
    "    t_data['FamilySize'] = get_family_size(t_data)\n",
    "\n",
    "    # Column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Fit the transformer and transform the data\n",
    "    transformed_data = preprocessor.fit_transform(t_data)\n",
    "\n",
    "    # Getting the column names for numerical features\n",
    "    numerical_cols = numerical_features\n",
    "\n",
    "    # Getting the column names for categorical features\n",
    "    categorical_cols = preprocessor.named_transformers_['cat']['encoder'].get_feature_names_out(categorical_features)\n",
    "\n",
    "    # Combine all column names\n",
    "    all_cols = list(numerical_cols) + list(categorical_cols)\n",
    "\n",
    "    # Add any remaining columns that were 'passed through'\n",
    "    pass_through_cols = [col for col in t_data.columns if col not in numerical_features + categorical_features]\n",
    "    all_cols.extend(pass_through_cols)\n",
    "\n",
    "    # Create the DataFrame\n",
    "    transformed_df = pd.DataFrame(transformed_data, columns=all_cols)\n",
    "\n",
    "    transformed_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "\n",
    "    # Set all dtypes to float\n",
    "    transformed_df = transformed_df.astype(float)\n",
    "\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Titanic data\n",
    "t_data = pd.read_csv('../../data_science_analytics_class/Data/titanic.csv')\n",
    "\n",
    "t_data_reduced = t_data.drop(columns=['Name', 'Ticket', 'Cabin'])\n",
    "titanic_data = t_data_reduced\n",
    "\n",
    "# map Embarked to numeric values and sex to numeric values\n",
    "titanic_data['Embarked'] = titanic_data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "titanic_data['Sex'] = titanic_data['Sex'].map({'male': 1, 'female': 0})\n",
    "\n",
    "# fill missing age with mean based on Pclass\n",
    "titanic_data['Age'] = titanic_data['Age'].fillna(titanic_data.groupby('Pclass')['Age'].transform('mean'))\n",
    "\n",
    "# drop missing embarked rows\n",
    "titanic_data.dropna(subset=['Embarked'], inplace=True)\n",
    "\n",
    "# Load transformed titanic data\n",
    "# titanic_data = titanic_data_transformed(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boston housing data\n",
    "b_data = pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_titanic_data = titanic_transformed(t_data)\n",
    "# # plot correlation matrix for transformed data with blue color map\n",
    "# plt.figure(figsize=(16, 6))\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.imshow(transformed_titanic_data.corr(), cmap='Blues', interpolation='nearest')\n",
    "# plt.xticks(np.arange(len(transformed_titanic_data.columns)), transformed_titanic_data.columns, rotation=90)\n",
    "# plt.yticks(np.arange(len(transformed_titanic_data.columns)), transformed_titanic_data.columns)\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33014/3006526191.py:79: FutureWarning: this method is deprecated in favour of `Styler.format(precision=..)`\n",
      "  styled_grid_results = grid_results_df.style.set_precision(3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_70359\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_70359_level0_col0\" class=\"col_heading level0 col0\" >Best Parameters</th>\n",
       "      <th id=\"T_70359_level0_col1\" class=\"col_heading level0 col1\" >Best Score</th>\n",
       "      <th id=\"T_70359_level0_col2\" class=\"col_heading level0 col2\" >Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_70359_level0_row0\" class=\"row_heading level0 row0\" >MLP</th>\n",
       "      <td id=\"T_70359_row0_col0\" class=\"data row0 col0\" >{'activation': 'tanh', 'hidden_layer_sizes': (50,), 'solver': 'adam'}</td>\n",
       "      <td id=\"T_70359_row0_col1\" class=\"data row0 col1\" >0.829</td>\n",
       "      <td id=\"T_70359_row0_col2\" class=\"data row0 col2\" >0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70359_level0_row1\" class=\"row_heading level0 row1\" >GradientBoosting</th>\n",
       "      <td id=\"T_70359_row1_col0\" class=\"data row1 col0\" >{'learning_rate': 0.1, 'n_estimators': 100}</td>\n",
       "      <td id=\"T_70359_row1_col1\" class=\"data row1 col1\" >0.829</td>\n",
       "      <td id=\"T_70359_row1_col2\" class=\"data row1 col2\" >0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70359_level0_row2\" class=\"row_heading level0 row2\" >k-NN</th>\n",
       "      <td id=\"T_70359_row2_col0\" class=\"data row2 col0\" >{'n_neighbors': 7, 'weights': 'uniform'}</td>\n",
       "      <td id=\"T_70359_row2_col1\" class=\"data row2 col1\" >0.798</td>\n",
       "      <td id=\"T_70359_row2_col2\" class=\"data row2 col2\" >0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70359_level0_row3\" class=\"row_heading level0 row3\" >Random Forest</th>\n",
       "      <td id=\"T_70359_row3_col0\" class=\"data row3 col0\" >{'max_depth': 10, 'n_estimators': 200}</td>\n",
       "      <td id=\"T_70359_row3_col1\" class=\"data row3 col1\" >0.826</td>\n",
       "      <td id=\"T_70359_row3_col2\" class=\"data row3 col2\" >0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70359_level0_row4\" class=\"row_heading level0 row4\" >SVM</th>\n",
       "      <td id=\"T_70359_row4_col0\" class=\"data row4 col0\" >{'C': 10, 'kernel': 'linear'}</td>\n",
       "      <td id=\"T_70359_row4_col1\" class=\"data row4 col1\" >0.832</td>\n",
       "      <td id=\"T_70359_row4_col2\" class=\"data row4 col2\" >0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70359_level0_row5\" class=\"row_heading level0 row5\" >XGBoost</th>\n",
       "      <td id=\"T_70359_row5_col0\" class=\"data row5 col0\" >{'learning_rate': 0.1, 'n_estimators': 100}</td>\n",
       "      <td id=\"T_70359_row5_col1\" class=\"data row5 col1\" >0.829</td>\n",
       "      <td id=\"T_70359_row5_col2\" class=\"data row5 col2\" >0.812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f06561f7a00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Feature matrix X, target vector y\n",
    "X = titanic_data.drop(['Survived'], axis=1)\n",
    "y = titanic_data['Survived']\n",
    "\n",
    "# Define the models and their corresponding parameter grids\n",
    "model_params = {\n",
    "    \"MLP\": {\n",
    "        \"model\": MLPClassifier(max_iter=10000),\n",
    "        \"params\": {\n",
    "            \"hidden_layer_sizes\": [(50,), (100,)],\n",
    "            \"activation\": [\"relu\", \"tanh\"],\n",
    "            \"solver\": [\"adam\", \"sgd\"]\n",
    "        }\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"learning_rate\": [0.01, 0.1]\n",
    "        }\n",
    "    },\n",
    "    \"k-NN\": {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_neighbors\": [3, 5, 7],\n",
    "            \"weights\": [\"uniform\", \"distance\"]\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"max_depth\": [None, 10, 20]\n",
    "        }\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(probability=True),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "            \"kernel\": [\"linear\", \"rbf\"]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"learning_rate\": [0.01, 0.1]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Split your data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Function to perform grid search and evaluate each model\n",
    "def grid_search_model(model_info, X_train, X_test, y_train, y_test):\n",
    "    grid_search = GridSearchCV(model_info['model'], model_info['params'], cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    scores = {\n",
    "        \"Best Parameters\": grid_search.best_params_,\n",
    "        \"Best Score\": grid_search.best_score_,\n",
    "        \"Test Accuracy\": accuracy_score(y_test, y_pred)\n",
    "        # Add other metrics as needed\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "# Evaluate each model\n",
    "grid_results = {}\n",
    "for name, model_info in model_params.items():\n",
    "    grid_results[name] = grid_search_model(model_info, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Convert results to DataFrame for display\n",
    "grid_results_df = pd.DataFrame(grid_results).T\n",
    "styled_grid_results = grid_results_df.style.set_precision(3)\n",
    "styled_grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix X, target vector y\n",
    "X = titanic_data.drop(['Survived'], axis=1)\n",
    "y = titanic_data['Survived']\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"MLP\": MLPClassifier(max_iter=1000),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "    \"k-NN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"XGBoost\": XGBClassifier()\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    scores = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "        \"Recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "        \"F1-Score\": f1_score(y_test, y_pred, average='weighted'),\n",
    "        # \"AUC-ROC\": roc_auc_score(y_test, y_score, average='weighted'),\n",
    "        # \"AUC-PR\": average_precision_score(y_test, y_score, average='weighted'), # Not directly supported for multiclass\n",
    "        \"Training Time\": training_time\n",
    "    }\n",
    "    # Create plots to add to dictionary\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_display_data = (cm, model.classes_)\n",
    "    \n",
    "    # Store the model and test set for ROC curve plotting\n",
    "    roc_data = (model, X_test, y_test)\n",
    "\n",
    "    plots = {\n",
    "        \"Confusion_Matrix\": cm_display_data,\n",
    "        \"ROC_Curve\": roc_data\n",
    "    }\n",
    "    return scores, plots\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "results = {}\n",
    "plots = {}\n",
    "for name, model in models.items():\n",
    "    grid_results[name], plots[name] = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    Highlight the maximum value in a Series with a green background.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: green' if cell else '' for cell in is_max]\n",
    "# Transpose the DataFrame for better readability\n",
    "results_df_transposed = results_df.T\n",
    "# Apply the highlighting style\n",
    "styled_results = results_df_transposed.style.apply(highlight_max).set_precision(3)\n",
    "styled_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot ROC curves in a separate figure\n",
    "fig_roc, ax_roc = plt.subplots(figsize=(10, 8))\n",
    "for model_name in models.keys():\n",
    "    model, X_test, y_test = plots[model_name][\"ROC_Curve\"]\n",
    "    RocCurveDisplay.from_estimator(model, X_test, y_test, ax=ax_roc, name=model_name)\n",
    "ax_roc.set_title('ROC Curves')\n",
    "ax_roc.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrices in another separate figure\n",
    "fig_cm, axes_cm = plt.subplots(nrows=2, ncols=3, figsize=(10, 8))  # Adjust nrows and ncols based on the number of models\n",
    "axes_cm = axes_cm.flatten()  # Flatten the array for easy iteration\n",
    "for i, model_name in enumerate(models.keys()):\n",
    "    cm, classes = plots[model_name][\"Confusion_Matrix\"]\n",
    "    # Plot the confusion matrix, with blue color\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes).plot(ax=axes_cm[i], cmap='Blues')\n",
    "    # ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes).plot(ax=axes_cm[i])\n",
    "    axes_cm[i].set_title(f\"{model_name}\")\n",
    "fig_cm.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titanic data testing zone\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Titanic data\n",
    "# t_data = pd.read_csv('../../data_science_analytics_class/Data/titanic.csv')\n",
    "\n",
    "# # save the tickets that appear more than once (i.e. group tickets)\n",
    "# _ = (t_data.Ticket.value_counts()>1).to_dict()\n",
    "# ls = []\n",
    "# for key in _:\n",
    "#     if _[key]==True:\n",
    "#         ls.append(key)\n",
    "# #extract the feature\n",
    "# t_data['Is_Group'] = t_data['Ticket'].apply(lambda x: 1 if x in(ls) else 0)\n",
    "\n",
    "# # create another dict containing the number of counts per each ticket\n",
    "# group_size = (t_data.Ticket.value_counts()).to_dict()\n",
    "# # extract the feature from the mapping\n",
    "# t_data['Group_Size'] = t_data['Ticket'].map(group_size).fillna(0)\n",
    "\n",
    "# # Function to extract title from name\n",
    "# def get_title(data):\n",
    "#     df = data.copy()\n",
    "#     df['Title'] = df.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
    "#     most_common_titles = df['Title'].value_counts().head(6).index.tolist()\n",
    "#     df.loc[~df['Title'].isin(most_common_titles), 'Title'] = 'Other'\n",
    "#     return df\n",
    "\n",
    "# get_title_transformer = FunctionTransformer(get_title)\n",
    "\n",
    "# # Function to calculate family size\n",
    "# def get_family_size(data):\n",
    "#     df = data.copy()\n",
    "#     df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "#     return df\n",
    "\n",
    "# get_family_size_transformer = FunctionTransformer(get_family_size)\n",
    "\n",
    "# # Numerical features pipeline\n",
    "# numerical_features = ['Age', 'Fare', 'FamilySize']\n",
    "# numerical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', MinMaxScaler())\n",
    "# ])\n",
    "\n",
    "# # Categorical features pipeline\n",
    "# categorical_features = ['Sex', 'Embarked', 'Title', 'Pclass']\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "# ])\n",
    "\n",
    "# # Column transformer\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numerical_transformer, numerical_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)\n",
    "#     ],\n",
    "#     remainder='drop'\n",
    "# )\n",
    "\n",
    "# # Classifier\n",
    "# knn_clf = KNN()\n",
    "\n",
    "# # Classification pipeline\n",
    "# pipe_clf = Pipeline(\n",
    "#     steps=[\n",
    "#         ('get_title', get_title_transformer),\n",
    "#         ('get_family_size', get_family_size_transformer),\n",
    "#         ('preprocessor', preprocessor),\n",
    "#         ('poly_features', PolynomialFeatures()),\n",
    "#         ('knn', knn_clf)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Feature matrix and target vector\n",
    "# X = t_data.drop(['Survived', 'PassengerId', 'Ticket', 'Cabin'], axis=1)\n",
    "# y = t_data['Survived']\n",
    "\n",
    "# # Train test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# # Grid search\n",
    "# param_grid = {\n",
    "#     'knn__n_neighbors': list(range(1, 25)),\n",
    "#     'knn__weights': ['uniform', 'distance'],\n",
    "#     'poly_features__degree': [1, 2]\n",
    "# }\n",
    "# grid_search = GridSearchCV(\n",
    "#     pipe_clf,\n",
    "#     param_grid,\n",
    "#     cv=5,\n",
    "#     scoring='accuracy',\n",
    "#     error_score='raise',\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best classifier\n",
    "# best_clf = grid_search.best_estimator_\n",
    "\n",
    "# # Predict and evaluate\n",
    "# y_pred = best_clf.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# # Draw confusion matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['died', 'survived'])\n",
    "\n",
    "# # Draw pr curve\n",
    "# y_score = best_clf.predict_proba(X_test)[:, 1]\n",
    "# precisions, recalls, thresholds = precision_recall_curve(y_test, y_score)\n",
    "# pr_display = PrecisionRecallDisplay(precision=precisions, recall=recalls)\n",
    "\n",
    "# # Draw roc auc curve\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "# roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)\n",
    "\n",
    "# # Plot all on one figure\n",
    "# fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# disp.plot(ax=ax[0], cmap='Blues')\n",
    "# pr_display.plot(ax=ax[1])\n",
    "# roc_display.plot(ax=ax[2])\n",
    "# # add auc score to plot\n",
    "# ax[2].text(0.5, 0.3, f'AUC: {roc_auc_score(y_test, y_score):.3f}', fontsize=12, ha='center')\n",
    "# ax[0].set_title('Confusion Matrix')\n",
    "# ax[1].set_title('Precision-Recall Curve')\n",
    "# ax[2].set_title('ROC Curve')\n",
    "# # draw chance line\n",
    "# ax[1].plot([0, 1], [0.5, 0.5], linestyle='--', color='black')\n",
    "# ax[2].plot([0, 1], [0, 1], linestyle='--', color='black')\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Result Analysis</u>\n",
    "- Present the comparison results in tables or graphs.\n",
    "- Statistical tests, if applicable, to establish significant differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Discussion</u>\n",
    "- Interpret the comparison findings.\n",
    "- Discuss where XGBoost outperforms or underperforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Conclusion</u>\n",
    "- Summarize key takeaways from the XGBoost exploration and model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices and Supporting Materials\n",
    "\n",
    "- Code snippets, Jupyter Notebook links, or GitHub repository.\n",
    "- Detailed tables and graphical representations of results.\n",
    "- Additional notes on the computational environment, data access, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- https://xgboost.readthedocs.io/en/latest/\n",
    "- https://www.kaggle.com/code/stuarthallows/using-xgboost-with-scikit-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sportsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
